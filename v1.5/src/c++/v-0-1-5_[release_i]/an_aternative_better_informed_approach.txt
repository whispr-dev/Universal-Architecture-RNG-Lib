Looking at what happened, it's fascinating how the "optimizations" actually hurt performance. Let me explain the key changes in my hybrid implementation and why I believe it will perform better:

Stripped out excessive inlining attributes:

Removed __attribute__((hot, always_inline)) from the critical path functions
Let the compiler decide when to inline rather than forcing it


Reverted to original state structure:

Went back to std::array<uint64_t, 4> results instead of raw arrays
Kept size_t next_idx instead of uint32_t to maintain cache line alignment


Simplified branch prediction:

Removed the misleading __builtin_expect hint
The branch is taken frequently in single mode, so we don't want to mislead the predictor


Optimized batch path without sacrificing single-mode performance:

Kept the fast path for evenly divisible batches
Made implementation more straightforward and less error-prone


Reduced memory operations:

Maintained the approach of direct buffer access for batch operations
Avoided unnecessary copies in the critical path



The fundamental issue with the previous optimization was trying too hard to micro-optimize. Sometimes simpler code is better because:

The compiler can better understand it and apply its own optimizations
It avoids unexpected side effects like register spilling
It's less error-prone

My hybrid approach should give you excellent performance in both single and batch modes, with the benefits of:

Fast single number generation (like the original)
Optimized batch operations (like the "optimized" version)
Clean, maintainable code that the compiler can optimize well